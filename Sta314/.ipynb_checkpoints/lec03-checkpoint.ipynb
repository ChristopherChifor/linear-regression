{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "EPhkfsEQ-dGM"
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nOCxwOtTKV7P"
   },
   "source": [
    "Let's consider a simple prediction problem. We are given an input $x \\in \\{0,1\\}$, and we are trying to predict a label $t \\in \\{0, 1\\}$. For this problem, we assume that the data generating distribution $p_{data}$ is given by the following table.\n",
    "\n",
    " &nbsp; | `x = 0` | `x = 1`\n",
    "--- | --- | ---\n",
    "**`t = 0`** | 9/20 | 2/20\n",
    "**`t = 1`** | 6/20 | 3/20\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "XUOgnJ1h-g4m"
   },
   "outputs": [],
   "source": [
    "def data_gen_joint(x, t):\n",
    "  \"\"\"Returns the data generating joint distribution p_data(x,t).\"\"\"\n",
    "  if x == 0 and t == 0:\n",
    "    return 9/20\n",
    "  elif x == 0 and t == 1:\n",
    "    return 6/20\n",
    "  elif x == 1 and t == 0:\n",
    "    return 2/20\n",
    "  elif x == 1 and t == 1:\n",
    "    return 3/20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D1Oz4oBDL-gX"
   },
   "source": [
    "Based on this joint distribution, we can compute the relevant marginal $p_{data}(x)$ and conditional $p_{data}(t | x)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "51ztAUIUB18S"
   },
   "outputs": [],
   "source": [
    "def data_gen_marginal(x):\n",
    "  \"\"\"Returns the data generating marginal distribution p_data(x).\"\"\"\n",
    "  px = 0\n",
    "  for t in [0, 1]:\n",
    "    px += data_gen_joint(x, t)\n",
    "  return px\n",
    "\n",
    "def data_gen_conditional(t, x):\n",
    "  \"\"\"Returns the data generating conditional distribution p_data(t | x).\"\"\"\n",
    "  return data_gen_joint(x, t) / data_gen_marginal(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KpI2mhO_MEWh"
   },
   "source": [
    "Let us consider predictors, which are functions $y : \\{0, 1\\} \\to \\{0, 1\\}$. It is not too hard to convince yourself that there are *only* 4 possible predictors:\n",
    "\n",
    " &nbsp; &nbsp; &nbsp; &nbsp;| $y_i(0)$ &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  &nbsp; |$y_i(1)$   &nbsp; &nbsp; &nbsp;  &nbsp; &nbsp; &nbsp; \n",
    "--- | --- | ---\n",
    "$y_1$ | 0 | 1\n",
    "$y_2$ | 1 | 0\n",
    "$y_3$ | 0 | 0\n",
    "$y_4$ | 1 | 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "UUkH00ShAII_"
   },
   "outputs": [],
   "source": [
    "def y_1(x):\n",
    "  return x\n",
    "\n",
    "def y_2(x):\n",
    "  return 1 - x\n",
    "\n",
    "def y_3(x):\n",
    "  return 0\n",
    "\n",
    "def y_4(x):\n",
    "  return 1\n",
    "\n",
    "all_predictors = [y_1, y_2, y_3, y_4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3V4KYLVtNseD"
   },
   "source": [
    "We will score predictors based on the 0-1 loss:\n",
    "\n",
    "$$L(y, t) = \\begin{cases} 0 & \\text{if } y = t\\\\0 &\\text{o.w.}\\end{cases}$$\n",
    "\n",
    "Given a predictor $y$, we can compute the expected loss on the data generating distribuition:\n",
    "\n",
    "$$\\mathcal{R}[y] = \\sum_{t \\in \\{0,1\\}} \\sum_{x \\in \\{0,1\\}} p_{data}(x,t) L(y(x), t)$$\n",
    "\n",
    "Given a predictor $y$ and a training set with $N$ data points, we can compute the average loss:\n",
    "\n",
    "$$\\hat{\\mathcal{R}}[y, \\mathcal{D}] = \\sum_{(x^{(i)}, t^{(i)}) \\in \\mathcal{D}} \\frac{1}{N} L(y(x^{(i)}), t^{(i)})$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "ugqzmVM8AGyX"
   },
   "outputs": [],
   "source": [
    "def loss(y, t):\n",
    "  \"\"\"The 0-1 loss function.\"\"\"\n",
    "  return int(y != t)\n",
    "\n",
    "def expected_loss(predictor, data_gen_dist):\n",
    "  \"\"\"Compute the expected loss of predictor : {0,1} -> {0,1}.\"\"\"\n",
    "  L = 0\n",
    "  for t in [0, 1]:\n",
    "    for x in [0, 1]:\n",
    "      L += data_gen_joint(x, t) * loss(predictor(x), t)\n",
    "  return L\n",
    "\n",
    "def average_loss(predictor, D):\n",
    "  \"\"\"Compute the expected loss of y : {0,1} -> {0,1}.\"\"\"\n",
    "  N = len(D)\n",
    "  L = 0\n",
    "  for (xi, ti) in D:\n",
    "    L += loss(predictor(xi), ti) / N\n",
    "  return L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ghjiBnoLO1JV"
   },
   "source": [
    "Let's evaluate the predictors $y_i$ on expected loss $\\mathcal{R}[y_i]$ under the data generating distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fB356CKtA41_",
    "outputId": "fad7b1be-1317-4641-bd32-646b14329deb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R[y_1]=0.40\n",
      "R[y_2]=0.60\n",
      "R[y_3]=0.45\n",
      "R[y_4]=0.55\n",
      "Which is the optimal predictor in terms of expected loss? y_2\n"
     ]
    }
   ],
   "source": [
    "predictor_losses = [(expected_loss(predictor, data_gen_joint), i) for (i, predictor) in enumerate(all_predictors)]\n",
    "for predictor_loss, i in predictor_losses:\n",
    "  print(f\"R[y_{i+1}]=\" + \"{:.2f}\".format(predictor_loss))\n",
    "\n",
    "# Report\n",
    "print(f\"Which is the optimal predictor in terms of expected loss? y_{max(predictor_losses)[1]+1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hBfR5BMXO53n"
   },
   "source": [
    "Notice that there is a *unique* optimal predictor: $y_2$. Let's consider how hard it is to *learn* this predictor from a finite training set. First, let's write a function for generating data sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "55SeXon9DtHV"
   },
   "outputs": [],
   "source": [
    "def sample_data_set(N):\n",
    "  \"\"\"Sample a data set according to the data generating distribution\"\"\"\n",
    "  data = []\n",
    "  for i in range(N):\n",
    "    # First, sample x.\n",
    "    px = [data_gen_marginal(0), data_gen_marginal(1)]\n",
    "    x = np.random.choice([0,1], p=px)\n",
    "\n",
    "    # Now we sample t given x.\n",
    "    pt_given_x = [data_gen_conditional(0, x), data_gen_conditional(1, x)]\n",
    "    t = np.random.choice([0,1], p=pt_given_x)\n",
    "\n",
    "    # Add (x,t) to training set\n",
    "    data.append((x, t))\n",
    "  return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lmKwy1T0PG-j"
   },
   "source": [
    "Let's sample a finite training set $\\mathcal{D}^{train}$, evaluate all of the predictors in terms of average loss $\\hat{\\mathcal{R}}[y_i, \\mathcal{D}^{train}]$ on the training set and pick the best one $\\hat{y}^{\\star}$. Which one did we pick?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XAKhtOkKBt-C",
    "outputId": "2e79fc69-5a8b-478f-b426-dba39d16830b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set\n",
      "x_0: 1  t_0: 1\n",
      "x_1: 0  t_1: 0\n",
      "x_2: 0  t_2: 1\n",
      "\n",
      "Let's evaluate the predictors on average training loss.\n",
      "y_1 average training loss: 0.33\n",
      "y_2 average training loss: 0.67\n",
      "y_3 average training loss: 0.67\n",
      "y_4 average training loss: 0.33\n",
      "\n",
      "Which is an optimal predictor in terms of average training loss? y_3\n",
      "What expected loss does it get? 0.45\n"
     ]
    }
   ],
   "source": [
    "N = 3\n",
    "train_set = sample_data_set(N)\n",
    "print(\"Training set\")\n",
    "for i, (x, t) in enumerate(train_set):\n",
    "  print(f\"x_{i}: {x}  t_{i}: {t}\")\n",
    "\n",
    "print(\"\\nLet's evaluate the predictors on average training loss.\")\n",
    "predictor_losses = [(average_loss(predictor, train_set), i) for (i, predictor) in enumerate(all_predictors)]\n",
    "for predictor_loss, i in predictor_losses:\n",
    "  print(f\"y_{i+1} average training loss: \" + \"{:.2f}\".format(predictor_loss))\n",
    "\n",
    "# This code finds the predictor with the minimum training loss\n",
    "predictor = all_predictors[max(predictor_losses)[1]]\n",
    "which_predictor = max(predictor_losses)[1]+1\n",
    "\n",
    "# Report\n",
    "print(f\"\\nWhich is an optimal predictor in terms of average training loss? y_{which_predictor}\")\n",
    "print(\"What expected loss does it get? {:.2f}\".format(expected_loss(predictor, data_gen_joint)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LJr15nS8PflB"
   },
   "source": [
    "If you run the previous cell a few times, you'll see that the best predictor $\\hat{y}^{\\star}$ is *random*, because our training set was *random*. Let's automate this process and see *how often* we pick the optimal predictor $\\hat{y}^{\\star} = y_2$ from random training sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "XHjY1Ff-IUT-"
   },
   "outputs": [],
   "source": [
    "def pick_predictor(train_set):\n",
    "  \"\"\"Pick the best predictor on this train set.\"\"\"\n",
    "  predictor_losses = [(average_loss(predictor, train_set), i) for (i, predictor) in enumerate(all_predictors)]\n",
    "  i = max(predictor_losses)[1]\n",
    "  return all_predictors[i], i+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5R__iH7jQs-R"
   },
   "source": [
    "With the code below you can vary the size of training sets $N$ and the number of random training sets that we sample $m$ to estimate the probability of picking the optimal predictor. Notice, as $N \\to \\infty$, we end up always picking $y_2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tZNADlrcIlOw",
    "outputId": "5c8fcc63-9f06-4acc-b6b8-dd369ae1410b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best predictor on train set 0 is y_2 and it has expected loss R[y_2]=0.60\n",
      "Best predictor on train set 1 is y_4 and it has expected loss R[y_4]=0.55\n",
      "Best predictor on train set 2 is y_4 and it has expected loss R[y_4]=0.55\n",
      "Best predictor on train set 3 is y_4 and it has expected loss R[y_4]=0.55\n",
      "Best predictor on train set 4 is y_3 and it has expected loss R[y_3]=0.45\n",
      "Best predictor on train set 5 is y_4 and it has expected loss R[y_4]=0.55\n",
      "Best predictor on train set 6 is y_4 and it has expected loss R[y_4]=0.55\n",
      "Best predictor on train set 7 is y_2 and it has expected loss R[y_2]=0.60\n",
      "Best predictor on train set 8 is y_1 and it has expected loss R[y_1]=0.40\n",
      "Best predictor on train set 9 is y_4 and it has expected loss R[y_4]=0.55\n",
      "\n",
      "Over 10 training sets of size 4, we picked the optimal predictor y_2 20.0 percent of the time\n"
     ]
    }
   ],
   "source": [
    "N = 4\n",
    "m = 10\n",
    "\n",
    "how_often_optimal = 0\n",
    "for j in range(m):\n",
    "  # Sampling a new training set of size N\n",
    "  train_set = sample_data_set(N)\n",
    "\n",
    "  # Pick the best predictor\n",
    "  predictor, which_predictor = pick_predictor(train_set)\n",
    "  how_often_optimal += (which_predictor == 2)\n",
    "    \n",
    "  # Report\n",
    "  print((f\"Best predictor on train set {j} is \" +\n",
    "        f\"y_{which_predictor} and it has expected loss R[y_{which_predictor}]=\" + \n",
    "        \"{:.2f}\".format(expected_loss(predictor, data_gen_joint))))\n",
    "\n",
    "# Report\n",
    "print((f\"\\nOver {m} training sets of size {N}, \"+\n",
    "       f\"we picked the optimal predictor y_2 {how_often_optimal/m * 100} percent of the time\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's see how good of an approximation we can form of the expected loss $\\hat{\\mathcal{R}}[\\hat{y}^{\\star}, \\mathcal{D}^{test}] \\approx \\mathcal{R}[\\hat{y}^{\\star}]$ using test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best training set predictor y_4 has expected loss 0.55 and average test loss 0.54 on test set of size 1000.\n"
     ]
    }
   ],
   "source": [
    "train_N = 4\n",
    "test_N = 1000\n",
    "test_set = sample_data_set(test_N)\n",
    "\n",
    "# Sampling a new training set of size N\n",
    "train_set = sample_data_set(train_N)\n",
    "\n",
    "# Pick the best predictor\n",
    "predictor, which_predictor = pick_predictor(train_set)\n",
    "\n",
    "# Report\n",
    "print((f\"Best training set predictor y_{which_predictor} \"+\n",
    "       \"has expected loss {:.2f} and \".format(expected_loss(predictor, data_gen_joint)) +\n",
    "       \"average test loss {:.2f} on test set of size {}.\".format(average_loss(predictor, test_set), test_N)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "lec03.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
