---
title: "STA238 - Winter 2021"
author: "Group 200 Christopher Chifor and Parssa Kyanzadeh"
date: March 19, 2021
subtitle: Assignment 4
output:
  pdf_document: default
---

```{r, include=FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
library(openintro)
library(tidyverse)
```


# Part 1
## Step 1 (Mathematical Justification)

<Type in your assumptions here.>

Firstly, assume that $X_1,...,X_n \stackrel{iid}{\sim} Normal(\mu = 0, \sigma^2)$. Our goal is to find the Maximum Likelihood Estimator of $\sigma^2$. We also know that the probability function is: $f(x)= \frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{1}{2}(\frac{x}{\sigma})^2}$. Where $\mu = 0$. So we have:

$$
\begin{aligned}
L(\mu=0, \sigma^2|X_1, \dots, X_n)&= L(0, \sigma^2 | X_1) \dots L(0, \sigma^2 | X_n)\\
L(\mu=0, \sigma^2|X_1, \dots, X_n)&= f_{\sigma^2}(x_1,\dots x_n)\\
L(\mu=0, \sigma^2|X_1, \dots, X_n)&=f_{\sigma^2}(x_1)\cdot f_{\sigma^2}(x_2) \dots f_{\sigma^2}(x_n)
\end{aligned}
$$
We can do this due to the fact that our realisations of random variables are independent. Substituting for each we get:
$$
\begin{aligned}
L(\mu=0, \sigma^2|X_1, \dots, X_n)&= \bigg(\frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{1}{2}(\frac{x_1}{\sigma})^2} \bigg)\cdot \bigg(\frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{1}{2}(\frac{x_2}{\sigma})^2} \bigg)\dots \bigg(\frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{1}{2}(\frac{x_n}{\sigma})^2} \bigg)\\
L(\mu=0, \sigma^2|X_1, \dots, X_n)&=\bigg(\frac{1}{\sigma\sqrt{2\pi}}\bigg)^n \bigg(e^{-\frac{1}{2} (\frac{x_1}{\sigma})^2 -\frac{1}{2} (\frac{x_2}{\sigma})^2 - \dots - \frac{1}{2} (\frac{x_n}{\sigma})^2}\bigg)\\
L(\mu=0, \sigma^2|X_1, \dots, X_n)&=\bigg(\frac{1}{\sigma\sqrt{2\pi}}\bigg)^n e^{-\frac{1}{2\sigma^2} \bigg(\sum_{i=1}^{n} x_i^2\bigg)}
\end{aligned}
$$
Thus, we have $L(\sigma^2) = \bigg(\frac{1}{\sigma\sqrt{2\pi}}\bigg)^n e^{-\frac{1}{2\sigma^2} \bigg(\sum_{i=1}^{n} x_i^2\bigg)}$. We can now take the loglikelihood of this as follows:
$$
\begin{aligned}
\mathcal{l}(\sigma^2) &= ln(L(\sigma^2))\\
\mathcal{l}(\sigma^2) &= ln\Bigg(\bigg(\frac{1}{\sigma\sqrt{2\pi}}\bigg)^n e^{-\frac{1}{2\sigma^2} \bigg(\sum_{i=1}^{n} x_i^2\bigg)}\Bigg)\\
\mathcal{l}(\sigma^2) &= ln\Bigg(\bigg(\frac{1}{\sigma\sqrt{2\pi}}\bigg)^n\Bigg) + ln\bigg( e^{-\frac{1}{2\sigma^2} \bigg(\sum_{i=1}^{n} x_i^2\bigg)} \bigg)\\
\mathcal{l}(\sigma^2) &= n ln\bigg(\frac{1}{\sigma\sqrt{2\pi}}\bigg) - {\frac{1}{2\sigma^2} \sum_{i=1}^{n} x_i^2}\\
\end{aligned}
$$
Now we can proceed with finding the MLE of $\sigma^2$ as such:
$$
\begin{aligned}
\frac{\partial l}{\partial \sigma^2} &= -n + \frac{\sum_{i=1}^{n} x_i^2}{\sigma^2}\\
\frac{\partial l}{\partial \sigma^2} &= -n + \frac{1}{\sigma^2}\bigg(\sum_{i=1}^{n} x_i^2 \bigg)
\end{aligned}
$$
So, to find the maximum of sigma^2 we need to set the first derivative equal to 0.

$$
\begin{aligned}
\frac{\partial l}{\partial \sigma^2} &= 0 = -n + \frac{\sum_{i=1}^{n} x_i^2}{\sigma^2}\\
n &= \frac{\sum_{i=1}^{n} x_i^2}{\sigma^2}\\
\sigma^2 &= \frac{1}{n}\sum_{i=1}^{n} x_i^2\\
\hat{\sigma}_{MLE}^2 &= \frac{1}{n}\sum_{i=1}^{n} x_i^2
\end{aligned}
$$
Now this could either be a maximum or a minimum. We need it to be a maximum so let's do the second derivative test. We get:

$$
\begin{aligned}
\frac{\partial^2 l}{\partial (\sigma^2)^2} &= -\frac{1}{\sigma^3} \sum_{i=1}^{n} x_i^2\\
\frac{\partial^2 l}{\partial (\sigma^2)^2} &< 0
\end{aligned}
$$
Since our second derivative is negative, by the second derivative test we can conclude that our MLE is indeed a maximum.


<Type your derivations of the MLE of $\sigma^2$ here. Be sure to identify the likelihood and loglikelihood functions.>

<Be sure to use the second derivative test.>

<I have provided some useful LaTeX coding examples below.> 

The first is an example of inline math code. To write math text in line with the text we surround the math text (LaTeX code) by one pair of dollar signs (\$).The sample variance formula is $S^2 = \frac{1}{n-1} \sum_{i=1}^n(X_i - \bar{X})^2$. 

The two estimators written on individual lines are done so by putting my math text (LaTeX code) in two sets of dollar signs (\$). See here: $$T_1 = S^2 = \frac{1}{n-1} \sum_{i=1}^n(X_i - \bar{X})^2 $$ $$T_2 = S^{2}_* = \frac{1}{n} \sum_{i=1}^n(X_i - \bar{X})^2 $$ 

Another useful line is $X_1,...,X_n \stackrel{iid}{\sim} Normal(\mu = 0, \sigma^2)$.

Try knitting this to see what it looks like!

## Step 2 (Simulation Justification)


<Here you can briefly explain your simulation>

```{r, include = TRUE}

## Here you can code your simulation and create the 3 plots.

## Here is some starter code for the simulation:

set.seed(123) # Set this seed to be the last 3 digits of your student number.
n = 50 #pre-specify your sample size
sigma_sq = 1

## Simulating from Normal
rnorm(n, mean=0, sd=sqrt(sigma_sq))


## Create your plots below. (I recommend using ggplot) 


## Remember the patchwork package might be helpful.


```

<Here you should describe the results of the simulation and the plots.>

<Here you should state your conclusions regarding the trade-offs of the two estimators $T_1$ and $T_2$.>


<Here is an example of one of the plots. In this example the data is Exponential (with mean parameter, $\theta$) and we are looking at the likelihood evaluated for $\hat\theta$ as the sample mean and sample median. The idea is to produce two plots like this, but for Normal data at the estimators specified previously (one for small $\sigma^2$, one for large $\sigma^2$):>

```{r}

set.seed(899)

T1 <- function(n, x) ((1/(n-1)) * sum(x^2))
T2 <- function(n, x) ((1/(n)) * sum(x^2))

n <- seq(10,100, 10)
sigma_sq = 1

likelihood_ratio <- rep(NA, length(n))

for(i in 1:length(n)){
  norm_data <- rnorm(n, mean=0, sd=sqrt(sigma_sq))
  L_T1 <- T1(n =n[i], norm_data)
  L_T2 <- T2(n =n[i], norm_data)
  likelihood_ratio[i] <- L_T1/L_T2
}


tibble(n, likelihood_ratio) %>% 
    ggplot(aes(x = n)) + 
    geom_line(aes(y = likelihood_ratio), color = "blue") + 
    labs(x = "n", y = "likelihood ratio", 
         title = "LikelihoodRatio for different n values for sigma squared = 1",
         subtitle = "T1 over T2",
         color = "" )
```

```{r}
set.seed(899)
T1 <- function(n, x) ((1/(n-1)) * sum(x^2))
T2 <- function(n, x) ((1/(n)) * sum(x^2))

n <- seq(10,100, 10)
sigma_sq1 = 100000

likelihood_ratio1 <- rep(NA, length(n))

for(i in 1:length(n)){
  norm_data1 <- rnorm(n, mean=0, sd=sqrt(sigma_sq1))
  L_T1_1 <- T1(n =n[i], norm_data1)
  L_T2_1 <- T2(n =n[i], norm_data1)
  likelihood_ratio1[i] <- L_T1_1/L_T2_1
}


tibble(n, likelihood_ratio) %>% 
    ggplot(aes(x = n)) + 
    geom_line(aes(y = likelihood_ratio1), color = "red") + 
    labs(x = "n", y = "likelihood ratio", 
         title = "LikelihoodRatio for different n values for sigma squared = 100000",
         subtitle = "T1 over T2",
         color = "" )
```

From our derivation in Step 1, we see that our population variance matches up with out MLE of $\sigma^2$. This makes sense as it should because the population variance is our $\hat{\sigma}_{MLE}^2$ value. This is our estimator $T_2$. From our graphs, with sample mean being 0, we can see that our two graphs are almost identical. This is because both of our estimators are so similar. Our first estimator which is our sample variance, we divide by $n-1$ and since our $n$ values: 10, 20,..., 100 are relatively large enough, we see almost no difference between our two estimators. When $n = 100$ for example, $T_1$ would be the sum of the first 100 realizations and we would divide by $(100-1) = 99$ which is virtually $100$, which is what we are dividing by for our $T_2$ estimator. Since our two estimators are virtually the same, we see our graphs our virtually the same. These results are in line with what we showed in Step 1. This is because when n is small, we have $T_1> T_2$, this is because for small values of $n$ our first estimator will be dividing by a smaller number (from the -1 part) thus yielding a larger value estimated. It follows that for small $n$, $T_2$ is smaller than $T_1$. When n is much larger we see that the ratio of $T_1$ and $T_2$ are converging to 1 which means the estimators are converging to a likelihood of 1. This is also clear from our Step 1 derivations.


<Please include some text describing the relationship between the two estimators with respect to likelihood. What is the plot telling us? Are these results in line with what was shown in Step 1?>


\newpage

# Part 2

```{r}
library(statcanR)

```



## Introduction

<Here you should have a few paragraphs of text introducing the problem, getting the reader interested/ready for the rest of the report.>

<Introduce terminology.>

<Highlight hypotheses.>

<Optional: You can also include a description of each section of this report as a last paragraph.>


## Data

<Type here a paragraph introducing the data, its context and the data collection process.>

```{r, include = FALSE}

# Here you can load in and clean the data (you may need to do the cleaning in a separate R script). 

# You may need additional chunks, in case you want to include some of the cleaning output.

```

<Type here a summary of the cleaning process.> 

<Remember, you may want to use multiple datasets here, if you do end up using multiple data sets, or merging the data, be sure to describe this in the cleaning process and be sure to discuss important aspects of all the data that you used.>

<Include a description of the important variables.> 

```{r, include=FALSE}

# Use this to calculate some summary measures. 

```


<Include a description of the numerical summaries. Remember you can use `r ` to use inline R code.>

```{r, echo = TRUE}

# Use this to create some plots. 

```

<Include a clear description of the plot(s). I would recommend one paragraph for each plot.> 


## Methods

<Include some text introducing the methodology.>

$$ include.your.mathematical.model.here.if.you.have.some.math.to.show $$
<Here you should describe a CI for the mean.>

<Here you should specify which CI you are using and why.>

<Here you should specifiy the confidence level. You may want to briefly elaborate on why you chose this confidence level. It should NOT be in order to get significant results - that would be an unethical practice.>


<Here you should describe the bootstrap.>


All analysis for this report was programmed using `R version 4.0.2`. 



## Results 


```{r, include = FALSE}

# Here you can calculate your CIS

# run a bootstrap.

# Here you can derive the CIs from the bootsrtap.



```

<Here you could present your results. You may want to put them into a well formatted table. Be sure that there is some text describing the results.>


<Note: Alternatively you can use the `knitr::kable` function to create a well formatted table from your code. See here: [https://rmarkdown.rstudio.com/lesson-7.html](https://rmarkdown.rstudio.com/lesson-7.html).>



<Remember you can use `r ` to use inline R code.>


```{r, include = FALSE}

# Here you can include some relevant visualizations.


```

<Include an explanation/interpretation of the visualizations. Make sure to comment on the appropriateness of the assumptions/results.>

## Conclusions

<Here you should give a summary of the Hypotheses, Methods and Results>

<Highlight Key Results.>

<Talk about big picture.>

<Comment on any Weaknesses.>

<Comment on Future Work/Next Steps>

<End with a concluding paragraph to wrap up the report.>


## Bibliography

1. Grolemund, G. (2014, July 16) *Introduction to R Markdown*. RStudio. [https://rmarkdown.rstudio.com/articles_intro.html](https://rmarkdown.rstudio.com/articles_intro.html). (Last Accessed: January 15, 2021) 

2. Dekking, F. M., et al. (2005) *A Modern Introduction to Probability and Statistics: Understanding why and how.* Springer Science & Business Media.

3.  Allaire, J.J., et. el. *References: Introduction to R Markdown*. RStudio. [https://rmarkdown.rstudio.com/docs/](https://rmarkdown.rstudio.com/docs/). (Last Accessed: January 15, 2021) 
